An algorithm whose worst case time complexity depends on numeric value of input (not number of inputs) is called Pseudo-polynomial algorithm.
There must be some first NP-Complete problem proved by definition of NP-Complete problems.  SAT (Boolean satisfiability problem) is the first NP-Complete problem proved by Cook

Bubble Sort -  It can be optimized by stopping the algorithm if inner loop didn’t cause any swap => no adjecent swap means already sorted array.

parent_index = (index - 1) >> 1

_siftup(pos)/heapify(pos) -> The child indices of heap index pos are already heaps, and we want to make a heap at index pos too.
_siftdown(startpos, pos) -> 'heap' is a heap at all indices >= startpos, except possibly for pos.  pos is the index of a leaf with a possibly out-of-order value.  Restore the heap invariant.

Problem with many binary search implementation:
int mid = (low + high) / 2 => leads to overflow for large values
use below instead
int mid = low + ((high - low) / 2);

Sorting waveform:
1) Traverse all even positioned elements of input array, and do following.
….a) If current element is smaller than previous odd element, swap previous and current.
….b) If current element is smaller than next odd element, swap next and current.

K-th smallest => Quick prtition => Avg O(n) / Worst O(n^2)

DP Notes:
Optimal Substructure: A given problems has Optimal Substructure Property if optimal solution of the given problem can be obtained by using optimal solutions of its subproblems. => this property of the problem reduces the search space from exponential to polynomial. Thus DPs can be solved in polynomial time.
Exponential Algo => Evalution(All Possible state of size n)
DP => Evluation(Problen size n) = Evalution(Subproblem size x: x < n) + decision weight
Solving subproblems solves the original problem(All the subproblem solution can be build in O(1) from bottom-up)
time(P) = time(subproblem) * #subproblem
Bottom-up perspective we can really keep track of(soln to only those previous problem which helps build future problems) what we need to store and save space.
For DP to work subproblem dependency should be acyclic

Devide&Conquer: Subproblems of the problem are impicitly solved T = 0 or solved in O(1). The time to solve the whole problem mostly depends upon time it takes to merge. If this time is smaller than the solving problem some other algorithmic way and subproblems are solved O(0) or O(1), then we should consider devide and conquer way. Time for D&C would mostly be calculated master theorem.

Masters Theorem:
https://www.geeksforgeeks.org/analysis-algorithm-set-4-master-method-solving-recurrences/
T(n) = aT(n/b) + f(n) |Where b>1 and a>=1
There are following three cases:
1. If f(n) = Θ(n^c) where c < Log(b)a then T(n) = Θ(n^Log(b)a) | work at leaves are more | total work = no of leaves = n^Log(b)a
2. If f(n) = Θ(n^c) where c = Log(b)a then T(n) = Θ(n^c*Log n) | #leaves = n (input size)| total work = work at leaves = height*work at each level
3.If f(n) = Θ(n^c) where c > Log(b)a then T(n) = Θ(f(n)) | work at root exceeds | total work = work at root > no of leaves = f(n)

Backtracking
Problems which are typically solved using backtracking technique have following property in common. These problems can only be solved by trying every possible configuration and each configuration is tried only once.Backtracking works in incremental way and is an optimization over the Naive solution where all possible configurations are generated and tried.

Hamiltonian Path in an undirected graph is a path that visits each vertex exactly *(once)

TODO:
What does it take to make all permutation computation iterartive. How is different from tree recursions.
